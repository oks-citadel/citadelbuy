# Sentry Alert Rule Templates for CitadelBuy
#
# This file contains pre-configured alert rule templates that can be imported
# into Sentry for consistent monitoring across all projects.
#
# Usage:
# 1. Navigate to Sentry Project → Alerts → Create Alert Rule
# 2. Use these templates as reference for configuration
# 3. Adjust thresholds based on your specific needs
#
# Last Updated: 2024-12-04

---
# =============================================================================
# CRITICAL ALERTS - Require Immediate Response
# =============================================================================

critical_high_error_rate:
  name: "[Production] High Error Rate - Immediate Action Required"
  type: error
  environment: production
  owner: platform-team

  conditions:
    metric: count
    threshold: 50
    time_window: 5 minutes
    comparison: ">="

  filters:
    - level: [error, fatal]
    - "NOT error.type:ValidationError"
    - "NOT error.type:BadRequestException"

  actions:
    - type: slack
      channel: "#incidents-critical"
      priority: urgent
      workspace: citadelbuy

    - type: pagerduty
      service: citadelbuy-backend
      severity: critical

    - type: email
      recipients:
        - engineering-leads@citadelbuy.com
        - oncall@citadelbuy.com

  notes: |
    This alert fires when error rate exceeds 50 errors in 5 minutes.
    Immediate investigation required as this indicates a production incident.

    Response Actions:
    1. Check Sentry dashboard for error patterns
    2. Review recent deployments
    3. Check system health dashboard
    4. Initiate incident response protocol if needed

---

critical_database_failures:
  name: "[Production] Database Connection Failures"
  type: error
  environment: production
  owner: infrastructure-team

  conditions:
    metric: count
    threshold: 10
    time_window: 2 minutes
    comparison: ">="

  filters:
    - "error.type:*Database* OR error.type:*Connection* OR error.type:*Prisma*"
    - level: [error, fatal]

  actions:
    - type: slack
      channel: "#incidents-critical"
      priority: urgent

    - type: pagerduty
      service: citadelbuy-infrastructure
      severity: critical

    - type: webhook
      url: "https://api.citadelbuy.com/internal/incidents/database"
      method: POST

  notes: |
    Database connection failures can cause cascading failures.
    This requires immediate attention from infrastructure team.

    Response Actions:
    1. Check database server status
    2. Verify connection pool settings
    3. Check for connection leaks
    4. Review database metrics (CPU, memory, connections)
    5. Consider scaling database if needed

---

critical_payment_failures:
  name: "[Production] Payment Processing Failures"
  type: error
  environment: production
  owner: payments-team

  conditions:
    metric: count
    threshold: 5
    time_window: 5 minutes
    comparison: ">="

  filters:
    - "transaction:*checkout* OR transaction:*payment* OR transaction:*stripe*"
    - level: [error, fatal]
    - "NOT http.status_code:400"

  actions:
    - type: slack
      channel: "#payments-team"
      priority: urgent
      mention: "@payments-oncall"

    - type: slack
      channel: "#incidents-critical"
      priority: urgent

    - type: pagerduty
      service: citadelbuy-payments
      severity: critical

    - type: email
      recipients:
        - payments-team@citadelbuy.com
        - finance-team@citadelbuy.com

  notes: |
    Payment failures directly impact revenue and customer experience.
    Critical priority - must be resolved immediately.

    Response Actions:
    1. Check Stripe dashboard for issues
    2. Verify payment gateway status
    3. Review recent payment flow changes
    4. Check for fraud prevention false positives
    5. Notify customer support team

---

critical_api_degradation:
  name: "[Production] API Response Time Degradation"
  type: metric_alert
  environment: production
  owner: platform-team

  conditions:
    metric: "avg(transaction.duration)"
    threshold: 2000
    unit: milliseconds
    time_window: 5 minutes
    comparison: ">="

  filters:
    - transaction.op: http.server
    - "NOT transaction:/health"
    - "NOT transaction:/metrics"

  actions:
    - type: slack
      channel: "#platform-alerts"
      priority: high

    - type: jira
      project: PLAT
      issue_type: Incident
      priority: High

    - type: email
      recipients:
        - platform-team@citadelbuy.com

  notes: |
    Sustained API response time > 2s indicates performance issues.
    May impact user experience and require scaling.

    Response Actions:
    1. Check APM dashboard for slow transactions
    2. Review database query performance
    3. Check external API dependencies
    4. Verify server resources (CPU, memory)
    5. Consider scaling if needed

---

# =============================================================================
# HIGH PRIORITY ALERTS - Urgent Response Required
# =============================================================================

high_auth_failures:
  name: "[Production] Authentication System Failures"
  type: error
  environment: production
  owner: auth-team

  conditions:
    metric: count
    threshold: 20
    time_window: 10 minutes
    comparison: ">="

  filters:
    - "transaction:*auth* OR transaction:*login* OR transaction:*jwt*"
    - level: [error, fatal]
    - "NOT http.status_code:401"

  actions:
    - type: slack
      channel: "#auth-team"
      priority: high

    - type: email
      recipients:
        - auth-team@citadelbuy.com
        - security-team@citadelbuy.com

  notes: |
    Authentication failures prevent users from accessing the platform.
    High priority - investigate within 2 hours.

    Response Actions:
    1. Check JWT service status
    2. Verify Redis session storage
    3. Review recent auth changes
    4. Check for DDoS or brute force attacks

---

high_checkout_errors:
  name: "[Production] Checkout Flow Errors"
  type: error
  environment: production
  owner: commerce-team

  conditions:
    metric: count
    threshold: 15
    time_window: 10 minutes
    comparison: ">="

  filters:
    - "transaction:*/checkout* OR transaction:*/cart*"
    - level: [error, fatal]

  actions:
    - type: slack
      channel: "#commerce-team"
      priority: high

    - type: email
      recipients:
        - commerce-team@citadelbuy.com

  notes: |
    Checkout errors directly impact conversion rate and revenue.
    Investigate and resolve within 2 hours.

    Response Actions:
    1. Check payment gateway integration
    2. Verify inventory system
    3. Review shipping calculation service
    4. Check for UI/UX issues

---

# =============================================================================
# WARNING ALERTS - Monitor and Investigate
# =============================================================================

warning_elevated_4xx_errors:
  name: "[Production] Elevated Client Error Rate (4xx)"
  type: metric_alert
  environment: production
  owner: platform-team

  conditions:
    metric: count
    threshold: 100
    time_window: 10 minutes
    comparison: ">="

  filters:
    - "http.status_code:[400 TO 499]"
    - "NOT http.status_code:401"
    - "NOT http.status_code:404"

  actions:
    - type: slack
      channel: "#platform-alerts"
      priority: medium

  notes: |
    Elevated 4xx errors may indicate client integration issues or API changes.
    Monitor and investigate patterns.

    Response Actions:
    1. Check which endpoints are affected
    2. Review API documentation accuracy
    3. Check for breaking changes in recent releases
    4. Notify API consumers if needed

---

warning_new_error_spike:
  name: "[Production] New Error Type Detected"
  type: new_issue
  environment: production
  owner: platform-team

  conditions:
    event_frequency: 10
    time_window: 5 minutes

  filters:
    - level: [error, fatal]
    - is: unresolved

  actions:
    - type: slack
      channel: "#platform-alerts"
      priority: medium

    - type: auto_assign
      team: platform-team

  notes: |
    New errors should be investigated to prevent escalation.
    Assign to appropriate team based on error context.

    Response Actions:
    1. Review error details and stack trace
    2. Check if related to recent deployment
    3. Assess user impact
    4. Create ticket if needs investigation

---

warning_performance_regression:
  name: "[Production] Performance Regression Detected"
  type: comparison_alert
  environment: production
  owner: performance-team

  conditions:
    metric: "p95(transaction.duration)"
    threshold: 25
    unit: percent
    comparison: "increased_by"
    comparison_period: 1 day
    time_window: 30 minutes

  filters:
    - transaction.op: http.server
    - "NOT transaction:/health"

  actions:
    - type: slack
      channel: "#performance-monitoring"
      priority: medium

    - type: github
      repo: citadelbuy/platform
      labels: [performance, regression]

    - type: email
      recipients:
        - performance-team@citadelbuy.com

  notes: |
    25% performance regression requires investigation.
    May indicate inefficient code or database queries.

    Response Actions:
    1. Compare performance before/after recent deployments
    2. Check for new N+1 queries
    3. Review added dependencies
    4. Profile slow transactions

---

warning_memory_leak:
  name: "[Production] Potential Memory Leak Detected"
  type: metric_alert
  environment: production
  owner: infrastructure-team

  conditions:
    metric: "avg(measurements.memory_used)"
    threshold: 85
    unit: percent
    time_window: 30 minutes
    comparison: ">="

  filters:
    - platform: node

  actions:
    - type: slack
      channel: "#infrastructure-alerts"
      priority: medium

  notes: |
    Sustained high memory usage may indicate a memory leak.
    Monitor and investigate if trend continues.

    Response Actions:
    1. Check memory trends over time
    2. Review recent code changes
    3. Analyze heap dumps if available
    4. Consider restarting affected services if critical

---

# =============================================================================
# FRONTEND SPECIFIC ALERTS
# =============================================================================

frontend_core_web_vitals:
  name: "[Production] Poor Core Web Vitals"
  type: metric_alert
  environment: production
  owner: frontend-team

  conditions:
    - metric: "p75(measurements.lcp)"
      threshold: 4000
      unit: milliseconds
      comparison: ">="

    - metric: "p75(measurements.fid)"
      threshold: 300
      unit: milliseconds
      comparison: ">="

    - metric: "p75(measurements.cls)"
      threshold: 0.25
      comparison: ">="

    logic: OR
    time_window: 1 hour

  filters:
    - transaction.op: pageload

  actions:
    - type: slack
      channel: "#frontend-team"
      priority: medium

    - type: email
      recipients:
        - frontend-team@citadelbuy.com
        - ux-team@citadelbuy.com

  notes: |
    Core Web Vitals impact SEO and user experience.
    Investigation recommended to maintain search rankings.

    Response Actions:
    1. Identify problematic pages
    2. Optimize images and assets
    3. Review JavaScript bundle size
    4. Check for layout shifts

---

frontend_javascript_errors:
  name: "[Production] High JavaScript Error Rate"
  type: metric_alert
  environment: production
  owner: frontend-team

  conditions:
    metric: count
    threshold: 50
    time_window: 10 minutes
    comparison: ">="

  filters:
    - platform: javascript
    - level: [error, fatal]
    - "NOT error.type:ChunkLoadError"

  actions:
    - type: slack
      channel: "#frontend-team"
      priority: high

  notes: |
    High JavaScript error rate impacts user experience.
    Check for browser compatibility or code issues.

    Response Actions:
    1. Check browser distribution
    2. Review recent frontend deployments
    3. Check session replays for context
    4. Test on affected browsers

---

frontend_rage_clicks:
  name: "[Production] Rage Clicks Detected"
  type: metric_alert
  environment: production
  owner: ux-team

  conditions:
    metric: count
    threshold: 20
    time_window: 1 hour
    comparison: ">="

  filters:
    - event.type: error
    - "has:replay_id"
    - "click.tag:*"

  actions:
    - type: slack
      channel: "#ux-team"
      priority: low
      include_replay: true

  notes: |
    Rage clicks indicate user frustration.
    Review session replays to identify UX issues.

    Response Actions:
    1. Watch session replays
    2. Identify common patterns
    3. Create UX improvement tickets
    4. Prioritize based on frequency

---

# =============================================================================
# MOBILE SPECIFIC ALERTS
# =============================================================================

mobile_crash_rate:
  name: "[Production] Elevated Mobile Crash Rate"
  type: metric_alert
  environment: production
  owner: mobile-team

  conditions:
    metric: "percentage(sessions_crashed, sessions)"
    threshold: 1.0
    unit: percent
    time_window: 1 hour
    comparison: ">="

  filters:
    - platform: [react-native, android, ios]

  actions:
    - type: slack
      channel: "#mobile-team"
      priority: high

    - type: pagerduty
      service: citadelbuy-mobile
      severity: high

  notes: |
    Crash rate above 1% requires immediate investigation.
    May affect app store ratings.

    Response Actions:
    1. Identify affected OS versions
    2. Review recent mobile releases
    3. Check crash reports and stack traces
    4. Consider hotfix if widespread

---

mobile_slow_startup:
  name: "[Production] Slow Mobile App Startup"
  type: metric_alert
  environment: production
  owner: mobile-team

  conditions:
    metric: "p75(transaction.duration)"
    threshold: 3000
    unit: milliseconds
    time_window: 30 minutes
    comparison: ">="

  filters:
    - transaction.op: app.start

  actions:
    - type: slack
      channel: "#mobile-team"
      priority: medium

  notes: |
    Slow app startup impacts first impression.
    Investigate and optimize startup performance.

    Response Actions:
    1. Profile app startup sequence
    2. Check for blocking operations
    3. Optimize asset loading
    4. Review initialization code

---

# =============================================================================
# BUSINESS CRITICAL ALERTS
# =============================================================================

business_low_conversion_rate:
  name: "[Production] Checkout Conversion Drop"
  type: metric_alert
  environment: production
  owner: commerce-team

  conditions:
    metric: "percentage(transactions(checkout-complete), transactions(checkout-start))"
    threshold: 50
    unit: percent
    comparison: "<"
    time_window: 4 hours

  actions:
    - type: slack
      channel: "#commerce-team"
      priority: high

    - type: email
      recipients:
        - commerce-team@citadelbuy.com
        - product-team@citadelbuy.com
        - cmo@citadelbuy.com

  notes: |
    Conversion rate drop indicates potential revenue loss.
    Immediate investigation to identify cause.

    Response Actions:
    1. Check for technical errors in checkout
    2. Review pricing/shipping calculations
    3. Check payment gateway status
    4. Analyze user drop-off points

---

business_api_quota_exceeded:
  name: "[Production] API Rate Limit Exceeded"
  type: metric_alert
  environment: production
  owner: platform-team

  conditions:
    metric: count
    threshold: 100
    time_window: 5 minutes
    comparison: ">="

  filters:
    - http.status_code: 429

  actions:
    - type: slack
      channel: "#platform-alerts"
      priority: medium

  notes: |
    Rate limit errors may indicate abuse or misconfigured clients.
    Review and adjust rate limits if needed.

    Response Actions:
    1. Identify source of requests
    2. Check for abuse or bot traffic
    3. Review rate limit policies
    4. Contact affected API consumers

---

# =============================================================================
# SCHEDULED REPORTS
# =============================================================================

report_weekly_error_summary:
  name: "Weekly Error Summary Report"
  type: scheduled_report
  schedule: "0 9 * * 1"  # Every Monday at 9 AM
  environment: production

  content:
    - top_errors:
        limit: 10
        sort_by: count

    - new_errors:
        time_range: 7 days

    - resolved_issues:
        time_range: 7 days

    - performance_trends:
        metrics:
          - p95(transaction.duration)
          - error_rate
          - apdex_score
        time_range: 7 days

  actions:
    - type: email
      recipients:
        - engineering-all@citadelbuy.com
        - product-team@citadelbuy.com

    - type: slack
      channel: "#weekly-metrics"

  notes: |
    Weekly summary report for engineering team.
    Review and discuss in weekly engineering meeting.

---

report_release_health:
  name: "Post-Release Health Report"
  type: release_report
  trigger: "24_hours_after_release"
  environment: production

  content:
    - new_errors:
        comparison: previous_release

    - performance_comparison:
        metrics:
          - p95(transaction.duration)
          - error_rate
          - throughput
        comparison: previous_release

    - user_adoption:
        metric: "count_unique(user)"

    - crash_rate:
        metric: "percentage(sessions_crashed, sessions)"
        comparison: previous_release

  actions:
    - type: email
      recipients:
        - release-managers@citadelbuy.com
        - engineering-leads@citadelbuy.com

    - type: slack
      channel: "#releases"

  notes: |
    Automated health report 24h after each production release.
    Review to ensure release stability.

---

# =============================================================================
# CONFIGURATION NOTES
# =============================================================================
#
# Alert Best Practices:
# 1. Start with conservative thresholds and tune based on actual patterns
# 2. Use appropriate time windows - shorter for critical, longer for warnings
# 3. Filter out noise (e.g., validation errors, 404s)
# 4. Include context in alert messages
# 5. Route alerts to appropriate teams/channels
# 6. Review and update alert rules monthly
#
# Threshold Tuning:
# - Monitor alert frequency and acknowledgment rates
# - Adjust if alerts fire too frequently (alert fatigue)
# - Tighten thresholds as system stability improves
# - Account for traffic patterns (business hours, holidays)
#
# Integration Setup:
# - Slack: Configure workspace and channels first
# - PagerDuty: Map services and escalation policies
# - Email: Use group emails, not individual addresses
# - Webhooks: Implement proper authentication
#
# Testing:
# - Test each alert rule after creation
# - Verify notification delivery to all channels
# - Ensure on-call rotation is correct
# - Perform monthly fire drills
#
# Documentation:
# - Document response procedures for each alert
# - Maintain runbooks for common issues
# - Update as system architecture changes
# - Share knowledge across teams
#
# =============================================================================
