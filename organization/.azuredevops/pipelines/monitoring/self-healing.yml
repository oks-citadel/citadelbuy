trigger: none

# This pipeline is triggered by webhooks from monitoring jobs
# or can be manually triggered with parameters

parameters:
  - name: action
    displayName: 'Self-Healing Action'
    type: string
    default: 'assess'
    values:
      - assess
      - restartPods
      - scaleDeployment
      - syncSecrets
      - rollbackDeployment
      - cleanupResources

  - name: namespace
    displayName: 'Kubernetes Namespace'
    type: string
    default: 'citadelbuy-prod'

  - name: deploymentName
    displayName: 'Deployment Name (if applicable)'
    type: string
    default: ''

  - name: targetReplicas
    displayName: 'Target Replicas (for scaling)'
    type: number
    default: 0

  - name: severity
    displayName: 'Issue Severity'
    type: string
    default: 'medium'
    values:
      - low
      - medium
      - high
      - critical

  - name: notifyTeam
    displayName: 'Send Team Notification'
    type: boolean
    default: true

variables:
  - group: monitoring-variables
  - group: kubernetes-config
  - name: aksClusterName
    value: 'citadelbuy-aks-prod'
  - name: aksResourceGroup
    value: 'citadelbuy-prod-rg'
  - name: keyVaultName
    value: 'citadelbuy-kv-prod'
  - name: maxRestartAttempts
    value: 3
  - name: incidentWorkItemType
    value: 'Incident'

pool:
  vmImage: 'ubuntu-latest'

stages:
  - stage: AssessIssue
    displayName: 'Assess Issue'
    condition: eq('${{ parameters.action }}', 'assess')
    jobs:
      - job: AnalyzeHealthStatus
        displayName: 'Analyze Current Health Status'
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Connect to AKS'
            inputs:
              azureSubscription: '$(azureServiceConnection)'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                az aks get-credentials \
                  --resource-group "$(aksResourceGroup)" \
                  --name "$(aksClusterName)" \
                  --overwrite-existing

          - task: Bash@3
            displayName: 'Assess System Health'
            inputs:
              targetType: 'inline'
              script: |
                echo "Assessing system health for namespace: ${{ parameters.namespace }}"

                # Check pod status
                kubectl get pods -n "${{ parameters.namespace }}" -o json > $(Build.ArtifactStagingDirectory)/pods.json

                TOTAL_PODS=$(cat $(Build.ArtifactStagingDirectory)/pods.json | jq '.items | length')
                RUNNING_PODS=$(cat $(Build.ArtifactStagingDirectory)/pods.json | jq '[.items[] | select(.status.phase == "Running")] | length')
                FAILED_PODS=$(cat $(Build.ArtifactStagingDirectory)/pods.json | jq '[.items[] | select(.status.phase == "Failed")] | length')
                CRASH_LOOP=$(cat $(Build.ArtifactStagingDirectory)/pods.json | jq '[.items[] | select(.status.containerStatuses[]? | .restartCount > 5)] | length')

                echo "Pod Status Summary:"
                echo "  Total: $TOTAL_PODS"
                echo "  Running: $RUNNING_PODS"
                echo "  Failed: $FAILED_PODS"
                echo "  CrashLoop: $CRASH_LOOP"

                # Determine recommended action
                RECOMMENDED_ACTION="none"

                if [ $CRASH_LOOP -gt 0 ]; then
                  RECOMMENDED_ACTION="restartPods"
                  echo "##[warning]Recommendation: Restart pods in CrashLoopBackOff"
                elif [ $FAILED_PODS -gt 2 ]; then
                  RECOMMENDED_ACTION="rollbackDeployment"
                  echo "##[warning]Recommendation: Consider rollback due to multiple failures"
                elif [ $RUNNING_PODS -lt $((TOTAL_PODS / 2)) ]; then
                  RECOMMENDED_ACTION="scaleDeployment"
                  echo "##[warning]Recommendation: Scale deployment to restore capacity"
                else
                  echo "System appears healthy, no action needed"
                fi

                echo "##vso[task.setvariable variable=recommendedAction;isOutput=true]$RECOMMENDED_ACTION"
                echo "##vso[task.setvariable variable=crashLoopCount;isOutput=true]$CRASH_LOOP"
            name: assess

          - template: ../templates/jobs/notify.yml
            parameters:
              channel: 'teams'
              webhookUrl: '$(teamsWebhookUrl)'
              message: |
                Self-Healing Assessment Complete

                Namespace: ${{ parameters.namespace }}
                Recommended Action: $(assess.recommendedAction)
                CrashLoop Pods: $(assess.crashLoopCount)
              severity: '${{ parameters.severity }}'

  - stage: RestartFailedPods
    displayName: 'Restart Failed Pods'
    condition: eq('${{ parameters.action }}', 'restartPods')
    jobs:
      - job: RestartPods
        displayName: 'Restart Problematic Pods'
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Connect to AKS'
            inputs:
              azureSubscription: '$(azureServiceConnection)'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                az aks get-credentials \
                  --resource-group "$(aksResourceGroup)" \
                  --name "$(aksClusterName)" \
                  --overwrite-existing

          - task: Bash@3
            displayName: 'Identify Problematic Pods'
            inputs:
              targetType: 'inline'
              script: |
                echo "Identifying pods that need restart in namespace: ${{ parameters.namespace }}"

                # Find pods in CrashLoopBackOff or with high restart counts
                kubectl get pods -n "${{ parameters.namespace }}" -o json | \
                  jq -r '.items[] | select(
                    (.status.containerStatuses[]? | .state.waiting.reason == "CrashLoopBackOff") or
                    (.status.containerStatuses[]? | .restartCount > 5)
                  ) | .metadata.name' > $(Build.ArtifactStagingDirectory)/pods-to-restart.txt

                POD_COUNT=$(cat $(Build.ArtifactStagingDirectory)/pods-to-restart.txt | wc -l)
                echo "Found $POD_COUNT pods needing restart"

                cat $(Build.ArtifactStagingDirectory)/pods-to-restart.txt

                echo "##vso[task.setvariable variable=podCount;isOutput=true]$POD_COUNT"
            name: identify

          - task: Bash@3
            displayName: 'Delete Problematic Pods'
            condition: and(succeeded(), gt(variables['identify.podCount'], 0))
            inputs:
              targetType: 'inline'
              script: |
                echo "Deleting problematic pods to trigger restart..."

                RESTART_COUNT=0

                while IFS= read -r pod_name; do
                  if [ -n "$pod_name" ]; then
                    echo "Deleting pod: $pod_name"

                    kubectl delete pod "$pod_name" -n "${{ parameters.namespace }}" --grace-period=30

                    if [ $? -eq 0 ]; then
                      echo "##[section]Successfully deleted pod: $pod_name"
                      RESTART_COUNT=$((RESTART_COUNT + 1))
                    else
                      echo "##[warning]Failed to delete pod: $pod_name"
                    fi
                  fi
                done < $(Build.ArtifactStagingDirectory)/pods-to-restart.txt

                echo "Restarted $RESTART_COUNT pods"
                echo "##vso[task.setvariable variable=restartedCount;isOutput=true]$RESTART_COUNT"
            name: restart

          - task: Bash@3
            displayName: 'Wait for Pods to Restart'
            condition: and(succeeded(), gt(variables['restart.restartedCount'], 0))
            inputs:
              targetType: 'inline'
              script: |
                echo "Waiting for pods to restart and become ready..."

                # Wait up to 5 minutes
                TIMEOUT=300
                ELAPSED=0
                INTERVAL=10

                while [ $ELAPSED -lt $TIMEOUT ]; do
                  # Check if all pods are running
                  NOT_READY=$(kubectl get pods -n "${{ parameters.namespace }}" -o json | \
                    jq '[.items[] | select(.status.phase != "Running" and .status.phase != "Succeeded")] | length')

                  if [ $NOT_READY -eq 0 ]; then
                    echo "##[section]All pods are now running!"
                    break
                  fi

                  echo "Waiting... ($ELAPSED/$TIMEOUT seconds) - $NOT_READY pods not ready"
                  sleep $INTERVAL
                  ELAPSED=$((ELAPSED + INTERVAL))
                done

                # Final status check
                kubectl get pods -n "${{ parameters.namespace }}"

                if [ $NOT_READY -gt 0 ]; then
                  echo "##[warning]Some pods are still not ready after $TIMEOUT seconds"
                fi

          - task: Bash@3
            displayName: 'Verify Health After Restart'
            condition: always()
            inputs:
              targetType: 'inline'
              script: |
                echo "Verifying pod health after restart..."

                kubectl get pods -n "${{ parameters.namespace }}" -o json > $(Build.ArtifactStagingDirectory)/pods-after-restart.json

                CRASH_LOOP_AFTER=$(cat $(Build.ArtifactStagingDirectory)/pods-after-restart.json | \
                  jq '[.items[] | select(.status.containerStatuses[]? | .state.waiting.reason == "CrashLoopBackOff")] | length')

                if [ $CRASH_LOOP_AFTER -gt 0 ]; then
                  echo "##[error]$CRASH_LOOP_AFTER pods still in CrashLoopBackOff after restart"
                  echo "##vso[task.setvariable variable=healingSuccess;isOutput=true]false"
                else
                  echo "##[section]Pod restart successful - no CrashLoopBackOff detected"
                  echo "##vso[task.setvariable variable=healingSuccess;isOutput=true]true"
                fi
            name: verify

          - template: ../templates/jobs/notify.yml
            parameters:
              channel: 'teams'
              webhookUrl: '$(teamsWebhookUrl)'
              message: |
                Pod Restart Completed

                Namespace: ${{ parameters.namespace }}
                Pods Restarted: $(restart.restartedCount)
                Success: $(verify.healingSuccess)
              severity: '${{ parameters.severity }}'

  - stage: ScaleDeployment
    displayName: 'Scale Deployment'
    condition: eq('${{ parameters.action }}', 'scaleDeployment')
    jobs:
      - job: AdjustReplicas
        displayName: 'Adjust Replica Count'
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Connect to AKS'
            inputs:
              azureSubscription: '$(azureServiceConnection)'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                az aks get-credentials \
                  --resource-group "$(aksResourceGroup)" \
                  --name "$(aksClusterName)" \
                  --overwrite-existing

          - task: Bash@3
            displayName: 'Get Current Replica Count'
            inputs:
              targetType: 'inline'
              script: |
                if [ -z "${{ parameters.deploymentName }}" ]; then
                  echo "##[error]Deployment name is required for scaling operation"
                  exit 1
                fi

                echo "Getting current replica count for: ${{ parameters.deploymentName }}"

                CURRENT_REPLICAS=$(kubectl get deployment "${{ parameters.deploymentName }}" \
                  -n "${{ parameters.namespace }}" \
                  -o jsonpath='{.spec.replicas}')

                echo "Current replicas: $CURRENT_REPLICAS"
                echo "Target replicas: ${{ parameters.targetReplicas }}"

                echo "##vso[task.setvariable variable=currentReplicas;isOutput=true]$CURRENT_REPLICAS"
            name: getCurrent

          - task: Bash@3
            displayName: 'Scale Deployment'
            inputs:
              targetType: 'inline'
              script: |
                echo "Scaling deployment ${{ parameters.deploymentName }} to ${{ parameters.targetReplicas }} replicas"

                kubectl scale deployment "${{ parameters.deploymentName }}" \
                  -n "${{ parameters.namespace }}" \
                  --replicas=${{ parameters.targetReplicas }}

                if [ $? -eq 0 ]; then
                  echo "##[section]Successfully scaled deployment"
                else
                  echo "##[error]Failed to scale deployment"
                  exit 1
                fi

          - task: Bash@3
            displayName: 'Wait for Rollout'
            inputs:
              targetType: 'inline'
              script: |
                echo "Waiting for deployment rollout to complete..."

                kubectl rollout status deployment "${{ parameters.deploymentName }}" \
                  -n "${{ parameters.namespace }}" \
                  --timeout=5m

                if [ $? -eq 0 ]; then
                  echo "##[section]Deployment rollout completed successfully"
                else
                  echo "##[error]Deployment rollout failed or timed out"
                  exit 1
                fi

          - template: ../templates/jobs/notify.yml
            parameters:
              channel: 'teams'
              webhookUrl: '$(teamsWebhookUrl)'
              message: |
                Deployment Scaled

                Deployment: ${{ parameters.deploymentName }}
                Namespace: ${{ parameters.namespace }}
                Previous Replicas: $(getCurrent.currentReplicas)
                New Replicas: ${{ parameters.targetReplicas }}
              severity: 'info'

  - stage: SyncSecrets
    displayName: 'Sync Secrets from Key Vault'
    condition: eq('${{ parameters.action }}', 'syncSecrets')
    jobs:
      - job: ResyncSecrets
        displayName: 'Re-sync Secrets'
        steps:
          - checkout: none

          - task: AzureCLI@2
            displayName: 'Connect to AKS'
            inputs:
              azureSubscription: '$(azureServiceConnection)'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                az aks get-credentials \
                  --resource-group "$(aksResourceGroup)" \
                  --name "$(aksClusterName)" \
                  --overwrite-existing

          - task: Bash@3
            displayName: 'Trigger Secret Rotation'
            inputs:
              targetType: 'inline'
              script: |
                echo "Triggering secret rotation for namespace: ${{ parameters.namespace }}"

                # Find all pods using CSI secrets driver
                PODS_WITH_CSI=$(kubectl get pods -n "${{ parameters.namespace }}" -o json | \
                  jq -r '.items[] | select(.spec.volumes[]? | .csi.driver == "secrets-store.csi.k8s.io") | .metadata.name')

                if [ -z "$PODS_WITH_CSI" ]; then
                  echo "##[warning]No pods found using CSI secrets driver"
                else
                  echo "Found pods using CSI secrets:"
                  echo "$PODS_WITH_CSI"

                  # Delete pods to trigger secret refresh
                  for pod in $PODS_WITH_CSI; do
                    echo "Restarting pod to refresh secrets: $pod"
                    kubectl delete pod "$pod" -n "${{ parameters.namespace }}" --grace-period=30
                  done
                fi

          - task: AzureCLI@2
            displayName: 'Manually Sync Critical Secrets'
            inputs:
              azureSubscription: '$(azureServiceConnection)'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                echo "Syncing critical secrets from Key Vault: $(keyVaultName)"

                # Define critical secrets to sync
                CRITICAL_SECRETS="database-connection-string redis-connection-string api-key"

                for secret_name in $CRITICAL_SECRETS; do
                  echo "Syncing secret: $secret_name"

                  # Get secret from Key Vault
                  SECRET_VALUE=$(az keyvault secret show \
                    --vault-name "$(keyVaultName)" \
                    --name "$secret_name" \
                    --query "value" \
                    -o tsv 2>/dev/null)

                  if [ -n "$SECRET_VALUE" ]; then
                    # Update Kubernetes secret
                    kubectl create secret generic "$secret_name" \
                      --from-literal=value="$SECRET_VALUE" \
                      -n "${{ parameters.namespace }}" \
                      --dry-run=client -o yaml | \
                      kubectl apply -f -

                    echo "✓ Synced: $secret_name"
                  else
                    echo "⚠ Secret not found in Key Vault: $secret_name"
                  fi
                done

          - template: ../templates/jobs/notify.yml
            parameters:
              channel: 'teams'
              webhookUrl: '$(teamsWebhookUrl)'
              message: |
                Secrets Re-synced

                Namespace: ${{ parameters.namespace }}
                Key Vault: $(keyVaultName)
              severity: 'info'

  - stage: NotifyAndCreateIncident
    displayName: 'Notify Team and Create Incident'
    condition: and(always(), eq('${{ parameters.notifyTeam }}', true))
    jobs:
      - job: SendNotifications
        displayName: 'Send Team Notifications'
        steps:
          - checkout: none

          - task: PowerShell@2
            displayName: 'Prepare Notification Details'
            inputs:
              targetType: 'inline'
              script: |
                $actionDetails = @{
                  action = "${{ parameters.action }}"
                  namespace = "${{ parameters.namespace }}"
                  deployment = "${{ parameters.deploymentName }}"
                  severity = "${{ parameters.severity }}"
                  timestamp = Get-Date -Format "yyyy-MM-ddTHH:mm:ssZ"
                  pipelineUrl = "$(System.CollectionUri)$(System.TeamProject)/_build/results?buildId=$(Build.BuildId)"
                }

                Write-Host "Self-healing action executed:"
                $actionDetails | ConvertTo-Json | Write-Host

                $actionDetails | ConvertTo-Json | Out-File "$(Build.ArtifactStagingDirectory)/action-details.json"

          - template: ../templates/jobs/notify.yml
            parameters:
              channel: 'teams'
              webhookUrl: '$(teamsWebhookUrl)'
              message: |
                Self-Healing Action Executed

                Action: ${{ parameters.action }}
                Namespace: ${{ parameters.namespace }}
                Severity: ${{ parameters.severity }}

                View Details: $(System.CollectionUri)$(System.TeamProject)/_build/results?buildId=$(Build.BuildId)
              severity: '${{ parameters.severity }}'

          - template: ../templates/jobs/notify.yml
            parameters:
              channel: 'slack'
              webhookUrl: '$(slackWebhookUrl)'
              message: |
                *Self-Healing Alert*

                Action: `${{ parameters.action }}`
                Namespace: `${{ parameters.namespace }}`
                Severity: `${{ parameters.severity }}`
              severity: '${{ parameters.severity }}'

      - job: CreateIncident
        displayName: 'Create Incident Work Item'
        condition: or(eq('${{ parameters.severity }}', 'high'), eq('${{ parameters.severity }}', 'critical'))
        dependsOn: []
        steps:
          - checkout: none

          - task: PowerShell@2
            displayName: 'Create Work Item'
            inputs:
              targetType: 'inline'
              script: |
                Write-Host "Creating incident work item for critical issue..."

                $title = "Self-Healing Triggered: ${{ parameters.action }} in ${{ parameters.namespace }}"
                $description = @"
                Self-healing pipeline was triggered due to detected issues.

                **Details:**
                - Action: ${{ parameters.action }}
                - Namespace: ${{ parameters.namespace }}
                - Deployment: ${{ parameters.deploymentName }}
                - Severity: ${{ parameters.severity }}
                - Timestamp: $(Get-Date -Format 'yyyy-MM-dd HH:mm:ss UTC')

                **Pipeline:**
                $(System.CollectionUri)$(System.TeamProject)/_build/results?buildId=$(Build.BuildId)

                **Next Steps:**
                1. Review the self-healing action taken
                2. Verify system stability
                3. Investigate root cause
                4. Update runbooks if needed
                "@

                az boards work-item create `
                  --title "$title" `
                  --type "$(incidentWorkItemType)" `
                  --area "$(System.TeamProject)\Operations" `
                  --assigned-to "$(Build.RequestedForEmail)" `
                  --description "$description" `
                  --organization "$(System.CollectionUri)" `
                  --project "$(System.TeamProject)" `
                  --fields "Microsoft.VSTS.Common.Priority=1" "Microsoft.VSTS.Common.Severity=2 - High"

                Write-Host "##[section]Incident work item created"

      - job: UpdateRunbook
        displayName: 'Update Self-Healing Runbook'
        condition: succeeded()
        dependsOn: []
        steps:
          - checkout: none

          - task: PowerShell@2
            displayName: 'Log Action to Runbook'
            inputs:
              targetType: 'inline'
              script: |
                Write-Host "Logging self-healing action to runbook..."

                $logEntry = @{
                  timestamp = Get-Date -Format "yyyy-MM-ddTHH:mm:ssZ"
                  action = "${{ parameters.action }}"
                  namespace = "${{ parameters.namespace }}"
                  deployment = "${{ parameters.deploymentName }}"
                  severity = "${{ parameters.severity }}"
                  buildId = "$(Build.BuildId)"
                  triggeredBy = "$(Build.RequestedFor)"
                }

                $logEntry | ConvertTo-Json | Out-File "$(Build.ArtifactStagingDirectory)/runbook-entry.json"

                Write-Host "Runbook entry:"
                $logEntry | ConvertTo-Json | Write-Host

                # In production, this would append to a centralized runbook/log

          - task: PublishPipelineArtifact@1
            displayName: 'Publish Action Log'
            inputs:
              targetPath: '$(Build.ArtifactStagingDirectory)'
              artifact: 'self-healing-log'
              publishLocation: 'pipeline'
